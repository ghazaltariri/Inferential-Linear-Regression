# Inferential-Linear-Regression

Linear Regression is highly useful when it comes to a tool for inferential analysis. It's often used as predictive tools and for that, we don't need many statistical assumptions to do a great job.
When we use it for inferential, we need to check some statistical properties of the model.

These are the assumptions of the linear regression model:
* Observations are independent.
* Errors have constant conditional variance. We call this "Homoskedacity".
* Errors are normally distributed.

The inferential results of the regression model depends on these statistical assumptions, and they are correct only if these assumptions hold at least approximately.

The **predictions** of the linear regression model are not dependent on these assumptions, so if the goals are *purely* predictive, these assumptions are not strong concerns.

We need to understand p-values associated with beta coefficients and then interpret them to gain insight into the relationship between variables.

